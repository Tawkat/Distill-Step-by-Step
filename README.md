# Distilling-Step-by-Step

Implementation of [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301)

This code is re-implemented for **Detoxification** project. (_Details coming soon!_)

### Datasets Description

Three columns of the dataset are used: toxic (source), non_toxic (target), and explanation.

## Run Model

```diff
./train-t5-distill.sh
```
